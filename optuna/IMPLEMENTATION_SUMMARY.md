# è¶…å‚æ•°ä¼˜åŒ–å®ç°æ€»ç»“

## ä¿®æ”¹æ¦‚è§ˆ

ä¸ºäº†å®ç°RL Teacherçš„è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–ï¼Œè¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹ï¼š

### 1. æ ¸å¿ƒæ–‡ä»¶ä¿®æ”¹

#### train.py
- **ä¿®æ”¹**ï¼šè®©mainå‡½æ•°è¿”å›è®­ç»ƒçš„ç»¼åˆè¯„åˆ†
- **åŸå› **ï¼šOptunaéœ€è¦è·å–è®­ç»ƒç»“æœæ¥è¯„ä¼°å‚æ•°å¥½å
- **æ”¹åŠ¨**ï¼š`agent.train()` â†’ `best_reward = agent.train()` + `return best_reward`

#### penspin/algo/ppo/ppo_rl_teacher.py
æ·»åŠ äº†æˆåŠŸæ¡ˆä¾‹è¿½è¸ªå’Œç»¼åˆè¯„åˆ†è®¡ç®—ï¼š

1. **æ–°å¢è¿½è¸ªå™¨**ï¼ˆç¬¬209-212è¡Œï¼‰ï¼š
   ```python
   self.success_count = 0
   self.total_episodes = 0
   self.best_success_rate = 0.0
   ```

2. **ç»Ÿè®¡æˆåŠŸæ¡ˆä¾‹**ï¼ˆç¬¬756-760è¡Œï¼‰ï¼š
   ```python
   if len(done_indices) > 0:
       success_mask = self.current_rot_angle[done_indices] > 4.0
       self.success_count += success_mask.sum().item()
       self.total_episodes += len(done_indices)
   ```

3. **è®¡ç®—å’Œè®°å½•æˆåŠŸç‡**ï¼ˆç¬¬384-391è¡Œï¼‰ï¼š
   - è®¡ç®—å½“å‰æˆåŠŸç‡
   - æ›´æ–°æœ€ä½³æˆåŠŸç‡
   - åœ¨æ—¥å¿—ä¸­æ˜¾ç¤º
   - è®°å½•åˆ°TensorBoard

4. **è¿”å›ç»¼åˆè¯„åˆ†**ï¼ˆç¬¬445-451è¡Œï¼‰ï¼š
   ```python
   composite_score = self.best_rewards + 1000.0 * self.best_success_rate
   return composite_score
   ```

### 2. æ–°å¢æ–‡ä»¶

#### optuna/tune_teacher.py
å®Œæ•´çš„è¶…å‚æ•°ä¼˜åŒ–è„šæœ¬ï¼ŒåŒ…å«ï¼š
- åŸºäºTPEçš„è´å¶æ–¯ä¼˜åŒ–
- 20+ä¸ªè¶…å‚æ•°çš„æœç´¢ç©ºé—´å®šä¹‰
- è‡ªåŠ¨è¯•éªŒç®¡ç†å’Œç»“æœä¿å­˜
- å¯è§†åŒ–ç”Ÿæˆï¼ˆå‚æ•°é‡è¦æ€§ã€ä¼˜åŒ–å†å²ï¼‰

#### optuna/run_hpo.sh
ä¾¿æ·çš„å¯åŠ¨è„šæœ¬ï¼Œç®€åŒ–å‘½ä»¤è¡Œè°ƒç”¨

#### optuna/README_HPO.md
è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£ï¼ŒåŒ…æ‹¬ï¼š
- ä¼˜åŒ–ç­–ç•¥è¯´æ˜
- å‚æ•°è¯´æ˜
- ä½¿ç”¨æ–¹æ³•
- æ•…éšœæ’é™¤

#### optuna/EXAMPLES.md
å®ç”¨ç¤ºä¾‹æ–‡æ¡£ï¼ŒåŒ…æ‹¬ï¼š
- å¿«é€Ÿå¼€å§‹
- å¤šGPUå¹¶è¡Œ
- ç»“æœåˆ†æ
- å¸¸è§é—®é¢˜

## ä¼˜åŒ–ç­–ç•¥

### è¯„åˆ†æœºåˆ¶

**ç»¼åˆè¯„åˆ†å…¬å¼ï¼š**
```
composite_score = best_reward + 1000 * success_rate
```

**è®¾è®¡ç†å¿µï¼š**
1. **ä¸»è¦ä¾æ®**ï¼šbest_rewardï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³å¹³å‡å¥–åŠ±ï¼‰
2. **å·¨å¤§åŠ æˆ**ï¼šsuccess_rateï¼ˆæ—‹è½¬è§’åº¦>10å¼§åº¦çš„æ¯”ä¾‹ï¼‰Ã— 1000
3. **åŸå› **ï¼šæˆåŠŸæ¡ˆä¾‹éå¸¸ç½•è§ï¼Œä¸€æ—¦å‡ºç°å°±åº”è¯¥è¢«é«˜åº¦é‡è§†

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

1. **ä»¥rewardä¸ºä¸»**ï¼š
   - rewardæ˜¯æ›´å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡
   - åŒ…å«äº†å¤šä¸ªæ–¹é¢çš„ç»¼åˆè¡¨ç°
   - è®­ç»ƒç¨³å®šï¼Œä¸ä¼šå› ä¸ºå•ä¸€æŒ‡æ ‡è¯¯å¯¼

2. **æˆåŠŸæ¡ˆä¾‹åŠ æˆ**ï¼š
   - æ—‹è½¬>4å¼§åº¦æ˜¯çœŸæ­£çš„æˆåŠŸæ ‡å‡†
   - æƒé‡1000ç¡®ä¿ä»»ä½•æˆåŠŸæ¡ˆä¾‹éƒ½ä¼šæ˜¾è‘—æå‡è¯„åˆ†
   - ä¾‹å¦‚ï¼š0.01çš„æˆåŠŸç‡ä¼šè´¡çŒ®10åˆ†ï¼Œç›¸å½“äºrewardæå‡10

3. **å¹³è¡¡ç­–ç•¥**ï¼š
   - å¦‚æœæ²¡æœ‰æˆåŠŸæ¡ˆä¾‹ï¼Œä»ç„¶ä¼˜åŒ–reward
   - å¦‚æœæœ‰æˆåŠŸæ¡ˆä¾‹ï¼Œä¼šä¼˜å…ˆé€‰æ‹©è¿™äº›å‚æ•°
   - é¿å…äº†çº¯ç²¹è¿½æ±‚æˆåŠŸç‡è€Œå¿½ç•¥æ•´ä½“æ€§èƒ½

## ä¼˜åŒ–çš„è¶…å‚æ•°

åŸºäº `RL_TEACHER_PARAMETERS.md`ï¼Œä¼˜åŒ–ä»¥ä¸‹å‚æ•°ï¼š

### æ ¸å¿ƒç®—æ³•å‚æ•°ï¼ˆ8ä¸ªï¼‰
- learning_rate, weight_decay, gamma, tau
- e_clip, entropy_coef, critic_coef, kl_threshold

### è®­ç»ƒå‚æ•°ï¼ˆ3ä¸ªï¼‰
- mini_epochs, minibatch_size, grad_norm

### å¥–åŠ±å‚æ•°ï¼ˆ7ä¸ªï¼‰â­ æœ€é‡è¦
- angvelClipMax, angvelPenaltyThresï¼ˆç›´æ¥å½±å“æ—‹è½¬è¡Œä¸ºï¼‰
- rotate_reward_scale, torque_penalty_scale, work_penalty_scale
- position_penalty_scale, rotate_penalty_scale

å…±è®¡ **18ä¸ªè¶…å‚æ•°**

## ä½¿ç”¨æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šè¿è¡Œä¼˜åŒ–

```bash
# ç®€å•æ–¹å¼
bash optuna/run_hpo.sh 0 50

# æˆ–é«˜çº§æ–¹å¼
python optuna/tune_teacher.py --gpu 0 --n_trials 50 --max_steps 100000000
```

### ç¬¬äºŒæ­¥ï¼šç›‘æ§è¿›åº¦

```bash
# æŸ¥çœ‹GPUä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f outputs/LinkerHandHora/optuna_trial_*/training.log

# æŸ¥çœ‹TensorBoard
tensorboard --logdir outputs/LinkerHandHora/optuna_trial_*
```

### ç¬¬ä¸‰æ­¥ï¼šåˆ†æç»“æœ

```bash
# æŸ¥çœ‹æœ€ä½³å‚æ•°
cat optuna/best_params_teacher_ppo_hpo.txt

# æ‰“å¼€å¯è§†åŒ–
firefox optuna/param_importances_teacher_ppo_hpo.html
firefox optuna/optimization_history_teacher_ppo_hpo.html
```

### ç¬¬å››æ­¥ï¼šä½¿ç”¨æœ€ä½³å‚æ•°

```bash
# è¿›è¡Œå®Œæ•´è®­ç»ƒï¼ˆ500Mæ­¥ï¼‰
scripts/train_rl_teacher.sh 0 42 final_best \
    train.ppo.learning_rate=<æœ€ä½³å€¼> \
    train.ppo.gamma=<æœ€ä½³å€¼> \
    task.env.reward.angvelClipMax=<æœ€ä½³å€¼> \
    # ... å…¶ä»–æœ€ä½³å‚æ•°
```

## æŠ€æœ¯ç»†èŠ‚

### Optunaé‡‡æ ·å™¨
ä½¿ç”¨ **TPESampler**ï¼ˆTree-structured Parzen Estimatorï¼‰ï¼š
- åŸºäºè´å¶æ–¯ä¼˜åŒ–
- æ¯”éšæœºæœç´¢å’Œç½‘æ ¼æœç´¢æ›´é«˜æ•ˆ
- ä¼šæ ¹æ®å†å²ç»“æœæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ç»„å‚æ•°

### æ•°æ®æŒä¹…åŒ–
ä½¿ç”¨ **SQLiteæ•°æ®åº“**ï¼š
- æ‰€æœ‰è¯•éªŒç»“æœè‡ªåŠ¨ä¿å­˜
- å¯ä»¥éšæ—¶ä¸­æ–­å’Œæ¢å¤
- æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼ˆå¤šGPUï¼‰

### ç›‘æ§æŒ‡æ ‡
è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•ï¼š
- episode_rewardsï¼šå¹³å‡å¥–åŠ±
- total_rot_angleï¼šå¹³å‡æ—‹è½¬è§’åº¦
- **success_rate**ï¼šæˆåŠŸç‡ï¼ˆæœ€å…³é”®ï¼ï¼‰â­
- success_countï¼šæˆåŠŸæ¡ˆä¾‹æ•°é‡

## é¢„æœŸæ•ˆæœ

### ä¼˜åŒ–å‰ï¼ˆé»˜è®¤å‚æ•°ï¼‰
- best_reward: ~50-70
- success_rate: ~0
- mean_rot_angle: ~1-2 rad

### ä¼˜åŒ–åï¼ˆç†æƒ³æƒ…å†µï¼‰
- best_reward: ~100-150
- success_rate: ~0.01-0.05ï¼ˆ1%-5%çš„episodeæˆåŠŸï¼‰
- mean_rot_angle: ~2-3 radï¼ˆæ•´ä½“æå‡ï¼‰

**æ³¨æ„**ï¼šå³ä½¿åªæœ‰1%çš„æˆåŠŸç‡ï¼ˆ0.01ï¼‰ï¼Œç»¼åˆè¯„åˆ†ä¹Ÿä¼šæå‡10åˆ†ï¼

## è®¡ç®—èµ„æºéœ€æ±‚

### å•æ¬¡è¯•éªŒ
- æ—¶é—´ï¼š1-3å°æ—¶ï¼ˆå–å†³äºmax_stepsï¼‰
- æ˜¾å­˜ï¼š~6-10GB
- ç£ç›˜ï¼š~500MB-1GB

### å®Œæ•´ä¼˜åŒ–ï¼ˆ50æ¬¡è¯•éªŒï¼‰
- æ—¶é—´ï¼š2-6å¤©ï¼ˆå–å†³äºGPUæ•°é‡ï¼‰
- æ€»ç£ç›˜ï¼š~25-50GB
- æ¨èï¼šä½¿ç”¨3-4ä¸ªGPUå¹¶è¡Œ

## æ‰©å±•æ€§

### æ·»åŠ æ–°å‚æ•°
åœ¨ `tune_teacher.py` çš„ `objective` å‡½æ•°ä¸­æ·»åŠ ï¼š

```python
# ä¾‹å¦‚æ·»åŠ horizon_lengthå‚æ•°
horizon_length = trial.suggest_int("horizon_length", 8, 16)
hpo_overrides.append(f"train.ppo.horizon_length={horizon_length}")
```

### ä¿®æ”¹æœç´¢èŒƒå›´
æ ¹æ®åˆæ­¥ç»“æœè°ƒæ•´ï¼š

```python
# ä¾‹å¦‚ç¼©å°å­¦ä¹ ç‡èŒƒå›´
lr = trial.suggest_float("learning_rate", 2e-4, 8e-3, log=True)
```

### å¤šç›®æ ‡ä¼˜åŒ–
å¯ä»¥æ‰©å±•ä¸ºåŒæ—¶ä¼˜åŒ–å¤šä¸ªæŒ‡æ ‡ï¼š

```python
study = optuna.create_study(directions=["maximize", "maximize"])
return best_reward, success_rate  # è¿”å›å¤šä¸ªå€¼
```

## æ•…éšœå¤„ç†

### å¸¸è§é—®é¢˜

1. **OOMé”™è¯¯**
   - è§£å†³ï¼šå‡å°minibatch_sizeæˆ–num_actors

2. **è®­ç»ƒå‘æ•£**
   - è§£å†³ï¼šé™åˆ¶å­¦ä¹ ç‡ä¸Šç•Œï¼Œå¢åŠ grad_normè£å‰ª

3. **ä¼˜åŒ–è¿›åº¦æ…¢**
   - è§£å†³ï¼šå¤šGPUå¹¶è¡Œï¼Œå‡å°‘max_steps

4. **æ²¡æœ‰æˆåŠŸæ¡ˆä¾‹**
   - æ­£å¸¸ç°è±¡ï¼ç»§ç»­ä¼˜åŒ–ï¼Œå…³æ³¨rewardçš„æå‡

## æ–‡ä»¶æ¸…å•

```
optuna/
â”œâ”€â”€ tune_teacher.py           # ä¸»ä¼˜åŒ–è„šæœ¬ â­
â”œâ”€â”€ run_hpo.sh               # ä¾¿æ·å¯åŠ¨è„šæœ¬
â”œâ”€â”€ README_HPO.md            # è¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ EXAMPLES.md              # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ RL_TEACHER_PARAMETERS.md # å‚æ•°è¯´æ˜ï¼ˆåŸæœ‰ï¼‰
â”œâ”€â”€ hpo_teacher.db           # æ•°æ®åº“ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â”œâ”€â”€ best_params_*.txt        # æœ€ä½³å‚æ•°ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
â””â”€â”€ *.html                   # å¯è§†åŒ–å›¾è¡¨ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰
```

## æ€»ç»“

é€šè¿‡æœ¬æ¬¡å®ç°ï¼š

1. âœ… å®ç°äº†å®Œæ•´çš„è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–ç³»ç»Ÿ
2. âœ… ä»¥rewardä¸ºä¸»ï¼ŒæˆåŠŸç‡æä¾›å·¨å¤§åŠ æˆçš„è¯„åˆ†ç­–ç•¥
3. âœ… æ”¯æŒ18ä¸ªå…³é”®è¶…å‚æ•°çš„ä¼˜åŒ–
4. âœ… æä¾›äº†å®Œå–„çš„æ–‡æ¡£å’Œç¤ºä¾‹
5. âœ… æ”¯æŒå¤šGPUå¹¶è¡ŒåŠ é€Ÿ
6. âœ… æ•°æ®æŒä¹…åŒ–ï¼Œå¯éšæ—¶ä¸­æ–­æ¢å¤

ç°åœ¨å¯ä»¥å¼€å§‹ä¼˜åŒ–äº†ï¼ğŸš€

ä½¿ç”¨å»ºè®®ï¼š
1. å…ˆç”¨50Mæ­¥å¿«é€Ÿè¯•éªŒ30-50æ¬¡ï¼Œæ‰¾åˆ°å¤§è‡´æ–¹å‘
2. å†ç”¨200Mæ­¥ç²¾ç»†ä¼˜åŒ–20-30æ¬¡
3. æœ€åç”¨æœ€ä½³å‚æ•°è¿›è¡Œå®Œæ•´è®­ç»ƒï¼ˆ500Mæ­¥ï¼‰

ç¥è®­ç»ƒæˆåŠŸï¼
